{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_list=[\"http://www.etihad.com/en-us/about-us/etihad-news/\",\n",
    "         \"https://www.qatarairways.com/en/press-releases.html\",\n",
    "         \"https://www.ethiopianairlines.com/SA/EN/news#\",\n",
    "         \"https://www.emirates.com/media-centre/\"]\n",
    "keywords=[\"launches\",\"launch\",\"capacity\",\"routes\",\"destination\",\"offers\",\"deploy\",\"airbus\",\n",
    "          \"increases\",\"frequency\",\"flights\",\"book\",\"pay\",\"travel\",\"dubai\",\"route\",\"price\",\"rate\",\"fare\"]\n",
    "\n",
    "links_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "links_list_3=[]\n",
    "links_list_2=[]\n",
    "links_list_1=[]\n",
    "links_list_0=[]\n",
    "for urls in url_list:\n",
    "    if urls==url_list[3]:\n",
    "        for y in keywords:\n",
    "            x=\"search?query=\"+y\n",
    "            url=('https://www.emirates.com/media-centre/'+ x +'')\n",
    "            page=requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            divs = soup.find_all(\"div\",{\"class\":\"rls rls--no-preview-image\"})\n",
    "            for a in divs:\n",
    "                links=a.find(\"a\")\n",
    "                c=(links['title']).lower()\n",
    "                key = re.compile('|'.join(keywords))\n",
    "                if key.findall(c):\n",
    "                    links_list_3.append(c)\n",
    "                    \n",
    "    if urls==url_list[2]:\n",
    "        page=requests.get(urls)\n",
    "        soup=BeautifulSoup(page.content,'html.parser')\n",
    "        news=soup.find_all(\"li\",{\"class\":\"media\"})\n",
    "        for i in news:\n",
    "            links=i.find_all(\"h4\")\n",
    "            try:\n",
    "                for j in links:\n",
    "                    c= j.text.lower()\n",
    "                    key = re.compile('|'.join(keywords))\n",
    "                    if key.findall(c):\n",
    "                        links_list_2.append(c)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if urls==url_list[1]:\n",
    "        chrome_path = r\"E:\\FlynavaIntern\\chromedriver\"\n",
    "        driver = webdriver.Chrome(chrome_path)\n",
    "        driver.set_window_size(1920, 1080)\n",
    "        driver.get(urls)\n",
    "        driver.maximize_window()\n",
    "        time.sleep(5)\n",
    "        pixel=100\n",
    "        for i in range(60):\n",
    "            pixel +=100\n",
    "            driver.execute_script(\"window.scrollTo(0,\"+ str(pixel) +\");\")\n",
    "        time.sleep(3)\n",
    "        driver.execute_script(\"window.scrollTo(0,0);\")\n",
    "        driver.execute_script(\"window.scrollTo(0,600);\")\n",
    "        time.sleep(20)\n",
    "        table=driver.find_element_by_xpath('//*[@id=\"main\"]/div[2]/div/div[2]/div[2]/div[2]/div/div')\n",
    "        time.sleep(3)\n",
    "        count=table.find_elements_by_tag_name('header')\n",
    "\n",
    "        count2=0\n",
    "        for each in count:\n",
    "            count2 += 1\n",
    "#         print \"Number of offers :\",count2\n",
    "        for i in range(count2):\n",
    "            try:\n",
    "                a= driver.find_element_by_xpath('//*[@id=\"articleDiv_'+str(i)+'\"]/article/div[2]/header/h3')\n",
    "                b= (a.text).lower()\n",
    "                key = re.compile('|'.join(keywords))\n",
    "                if key.findall(b):\n",
    "                    links_list_1.append(b)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if urls==url_list[0]:\n",
    "        page=requests.get(urls)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        news=soup.find_all(\"h3\",{\"class\":\"linkHeader\"})\n",
    "        for i in news:\n",
    "            a=i.text\n",
    "            b=a.lower()\n",
    "            key = re.compile('|'.join(keywords))\n",
    "            if key.findall(b):\n",
    "                links_list_0.append(b)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "71\n",
      "80\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "print len(links_list_0)\n",
    "print len(links_list_1)\n",
    "print len(links_list_2)\n",
    "print len(links_list_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "# db = client[\"Urls_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(links_list_0)):\n",
    "#     a= links_list_0[i]\n",
    "#     doc1={\n",
    "#         \"Title\":a\n",
    "#     }\n",
    "    \n",
    "#     db.etihad_news.insert_one(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(links_list_1)):\n",
    "#     a= links_list_1[i]\n",
    "#     doc1={\n",
    "#         \"Title\":a\n",
    "#     }\n",
    "    \n",
    "#     db.qatar_news.insert_one(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(links_list_2)):\n",
    "#     a= links_list_2[i]\n",
    "#     doc1={\n",
    "#         \"Title\":a\n",
    "#     }\n",
    "    \n",
    "#     db.ethiopian_news.insert_one(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(links_list_3)):\n",
    "#     a= links_list_3[i]\n",
    "#     doc1={\n",
    "#         \"Title\":a\n",
    "#     }\n",
    "    \n",
    "#     db.emirates_news.insert_one(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "total=len(links_list_0)+len(links_list_1)+len(links_list_2)+len(links_list_3)\n",
    "print total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
